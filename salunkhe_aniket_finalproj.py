# -*- coding: utf-8 -*-
"""salunkhe_aniket_finalProj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GvIO_j8xo0GxkRtM_Df6Xmm7paQBIuVw

# Mushroom Binary Classification
## Name: Aniket Nitin Salunkhe
## UCID: as4593
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import warnings
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, KFold
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import time
warnings.filterwarnings("ignore")

# Load the dataset
file_path = 'mushroom_cleaned.csv'
data = pd.read_csv(file_path)

# Display dataset information
print("First 5 rows of the dataset:")
display(data.head())
print("\nDataset Info:")
data.info()

# Check for missing values
print("\nMissing Values in Dataset:")
print(data.isnull().sum())

# Dataset statistics
print("\nDataset Statistics:")
display(data.describe())

# Class distribution
sns.countplot(x='class', data=data, palette='coolwarm')
plt.title("Class Distribution")
plt.show()

# Separate features and target
X = data.drop(columns=['class'])
y = data['class']

# Standardize the dataset
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print("\nTraining and Testing Data Shapes:")
print(f"X_train: {X_train.shape} | y_train: {y_train.shape}")
print(f"X_test: {X_test.shape} | y_test: {y_test.shape}")

"""# Evaluation Metrics Description

To evaluate the performance of the models in this project, the following metrics are calculated:

1. **True Positives (TP):** The number of positive samples correctly classified as positive.
2. **True Negatives (TN):** The number of negative samples correctly classified as negative.
3. **False Positives (FP):** The number of negative samples incorrectly classified as positive.
4. **False Negatives (FN):** The number of positive samples incorrectly classified as negative.
5. **Total Positives (P):** The total number of actual positive samples in the dataset.
6. **Total Negatives (N):** The total number of actual negative samples in the dataset.

### Derived Metrics

1. **True Positive Rate (TPR):** Also called recall or sensitivity, it is calculated as  
   $ \text{TPR} = \frac{\text{TP}}{\text{P}} $.  
   It measures the proportion of actual positives correctly identified.

2. **True Negative Rate (TNR):** Also called specificity, it is calculated as  
   $ \text{TNR} = \frac{\text{TN}}{\text{N}} $.  
   It measures the proportion of actual negatives correctly identified.

3. **False Positive Rate (FPR):** The proportion of actual negatives incorrectly classified as positives, calculated as  
   $ \text{FPR} = \frac{\text{FP}}{\text{N}} $.

4. **False Negative Rate (FNR):** The proportion of actual positives incorrectly classified as negatives, calculated as  
   $ \text{FNR} = \frac{\text{FN}}{\text{P}} $.

5. **Precision:** The ratio of true positives to all predicted positives, calculated as  
   $ \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} $.

6. **F1 Score:** The harmonic mean of precision and recall, calculated as  
   $ \text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} $.

7. **Accuracy:** The overall proportion of correct predictions, calculated as  
   $ \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{P} + \text{N}} $.

8. **Error Rate:** The proportion of incorrect predictions, calculated as  
   $ \text{Error Rate} = 1 - \text{Accuracy} $.

9. **Balanced Accuracy:** The average of TPR and TNR, calculated as  
   $ \text{Balanced Accuracy} = \frac{\text{TPR} + \text{TNR}}{2} $.

10. **True Skill Statistic (TSS):** The difference between TPR and FPR, calculated as  
    $ \text{TSS} = \text{TPR} - \text{FPR} $.  
    It evaluates the skill of the classifier independent of class imbalance.

11. **Heidke Skill Score (HSS):** A metric assessing classifier accuracy while considering random chance.

12. **Brier Score:** Measures the mean squared error between predicted probabilities and the actual outcomes. Lower scores indicate better calibration.

13. **Brier Skill Score:** A normalized version of the Brier Score comparing the model to a baseline model (e.g., random guessing).

These metrics provide a comprehensive assessment of the model's performance, covering aspects such as precision, recall, and calibration.
"""

from sklearn.metrics import confusion_matrix, roc_auc_score
import numpy as np

# Function to calculate metrics
def calculate_metrics(y_true, y_pred, y_prob=None):
    cm = confusion_matrix(y_true, y_pred)
    TP = cm[1, 1]
    TN = cm[0, 0]
    FP = cm[0, 1]
    FN = cm[1, 0]

    # Basic metrics
    P = TP + FN
    N = TN + FP
    TPR = TP / P if P > 0 else 0
    TNR = TN / N if N > 0 else 0
    FPR = FP / N if N > 0 else 0
    FNR = FN / P if P > 0 else 0
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    f1 = 2 * precision * TPR / (precision + TPR) if (precision + TPR) > 0 else 0
    accuracy = (TP + TN) / (P + N) if (P + N) > 0 else 0
    error_rate = 1 - accuracy

    # Advanced metrics
    balanced_accuracy = (TPR + TNR) / 2
    tss = TPR + TNR - 1
    hss = 2 * (TP * TN - FP * FN) / ((P * (FP + TN)) + (N * (TP + FN))) if (P * (FP + TN) + N * (TP + FN)) > 0 else 0

    # Brier Score (requires probabilities)
    brier_score = np.mean((y_prob - y_true)**2) if y_prob is not None else None
    brier_skill_score = 1 - brier_score / np.var(y_true) if brier_score is not None else None

    # ROC AUC (requires probabilities)
    roc_auc = roc_auc_score(y_true, y_prob) if y_prob is not None else None

    return {
        'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN,
        'P': P, 'N': N,
        'TPR': TPR, 'TNR': TNR, 'FPR': FPR, 'FNR': FNR,
        'Precision': precision, 'F1': f1, 'Accuracy': accuracy, 'Error Rate': error_rate,
        'Balanced Accuracy': balanced_accuracy, 'TSS': tss, 'HSS': hss,
        'Brier Score': brier_score, 'Brier Skill Score': brier_skill_score,
        'ROC AUC': roc_auc  # Added ROC AUC metric
    }

"""# Random Forest Classification with Stratified K-Fold Cross-Validation

In this cell, we implement a Random Forest classification model and evaluate its performance using **Stratified K-Fold Cross-Validation**. This ensures that each fold maintains the same proportion of positive and negative samples, providing a more robust evaluation of the model's performance.

#### Process
1. **Data Preparation:**
   - Convert `X_train` and `y_train` into NumPy arrays to enable indexing and slicing for Stratified K-Fold.
   
2. **Stratified K-Fold Cross-Validation:**
   - The dataset is split into 10 folds using `StratifiedKFold` with shuffling for randomness and a fixed random state for reproducibility.
   - For each fold:
     - The training and validation subsets are created.
     - A Random Forest model with 100 estimators is trained on the training subset.
     - Predictions (`y_pred_k`) and probabilities (`y_prob_k`) are generated for the validation subset.

3. **Metric Calculation:**
   - Custom evaluation metrics are calculated for each fold using the `calculate_metrics` function, which computes metrics like accuracy, precision, recall, F1-score, Brier score, and more.

4. **Metrics Aggregation:**
   - Metrics for each fold are stored in a list and later converted into a DataFrame for analysis.
   - The mean of all folds is computed to derive the overall performance of the Random Forest model.

#### Output
- **Fold-wise Metrics:** A detailed breakdown of the evaluation metrics for each fold.
- **Average Metrics:** The mean values of the metrics across all folds, representing the overall performance of the Random Forest model.

The results highlight the model's ability to handle the classification task, showing metrics such as **accuracy, precision, recall, F1-score, and Brier scores**, which provide a comprehensive view of its strengths and areas for improvement.

"""

# Random Forest Model
rf_metrics = []
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

for train_idx, val_idx in kf.split(X_train, y_train):
    X_train_k, X_val_k = X_train[train_idx], X_train[val_idx]
    y_train_k, y_val_k = y_train.iloc[train_idx], y_train.iloc[val_idx]

    rf_model = RandomForestClassifier(random_state=42)
    rf_model.fit(X_train_k, y_train_k)
    y_pred_k = rf_model.predict(X_val_k)
    y_prob_k = rf_model.predict_proba(X_val_k)[:, 1]

    metrics = calculate_metrics(y_val_k, y_pred_k, y_prob_k)
    rf_metrics.append(metrics)

rf_metrics_df = pd.DataFrame(rf_metrics)
print("\nRandom Forest Metrics Per Fold:")
display(rf_metrics_df)

# Average Metrics for Random Forest
rf_avg_metrics = rf_metrics_df.mean()
print("\nRandom Forest Average Metrics:")
print(rf_avg_metrics)

"""# Naive Bayes Classification with Stratified K-Fold Cross-Validation

In this cell, we implement a **Naive Bayes classification model** and evaluate its performance using **Stratified K-Fold Cross-Validation**. This approach ensures consistent evaluation by maintaining the class distribution in each fold.

#### Process
1. **Data Preparation:**
   - Convert `X_train` and `y_train` into NumPy arrays for easy manipulation and indexing during cross-validation.

2. **Stratified K-Fold Cross-Validation:**
   - The dataset is split into 10 folds using `StratifiedKFold`, ensuring the class balance is preserved in each fold.
   - For each fold:
     - The training and validation subsets are created.
     - A **Multinomial Naive Bayes (MultinomialNB)** model is trained on the training subset.
     - Predictions (`y_pred_k`) and probability scores (`y_prob_k`) are generated for the validation subset.

3. **Metric Calculation:**
   - Custom metrics, such as accuracy, precision, recall, F1-score, Brier score, and more, are calculated for each fold using the `calculate_metrics` function.
   - Metrics for each fold are stored in a list.

4. **Metrics Aggregation:**
   - Metrics for all folds are compiled into a DataFrame for detailed analysis.
   - The average metrics across all folds are computed to summarize the overall performance of the Naive Bayes model.

#### Output
- **Fold-wise Metrics:** Displays the performance metrics for each fold, providing insights into the model's consistency across different splits.
- **Average Metrics:** Provides the mean values of the evaluation metrics across all folds, representing the overall capability of the Naive Bayes classifier.

#### Observations
The results include key metrics such as:
- **Accuracy, Precision, Recall, F1-score** to measure classification performance.
- **Brier Score and Brier Skill Score** to evaluate the model's probability calibration.

These metrics provide a comprehensive assessment of the Naive Bayes classifier's performance, highlighting its strengths and limitations in the sentiment analysis task.
"""

# Naive Bayes Model
nb_metrics = []
for train_idx, val_idx in kf.split(X_train, y_train):
    X_train_k, X_val_k = X_train[train_idx], X_train[val_idx]
    y_train_k, y_val_k = y_train.iloc[train_idx], y_train.iloc[val_idx]

    nb_model = GaussianNB()
    nb_model.fit(X_train_k, y_train_k)
    y_pred_k = nb_model.predict(X_val_k)
    y_prob_k = nb_model.predict_proba(X_val_k)[:, 1]

    metrics = calculate_metrics(y_val_k, y_pred_k, y_prob_k)
    nb_metrics.append(metrics)

nb_metrics_df = pd.DataFrame(nb_metrics)
print("\nNaive Bayes Metrics Per Fold:")
display(nb_metrics_df)

# Average Metrics for Naive Bayes
nb_avg_metrics = nb_metrics_df.mean()
print("\nNaive Bayes Average Metrics:")
print(nb_avg_metrics)

"""# K-Nearest Neighbors (KNN) Classification with K-Fold Cross-Validation

In this cell, we train and evaluate a **K-Nearest Neighbors (KNN)** model using **10-Fold Cross-Validation**. This approach splits the dataset into 10 subsets (folds) and evaluates the model's performance on each fold to ensure robust results.

#### Process
1. **Data Preparation:**
   - Convert `X_train` and `y_train` into NumPy arrays to facilitate efficient slicing and manipulation for K-Fold splits.

2. **K-Fold Cross-Validation:**
   - The dataset is split into 10 folds using `KFold`, with shuffling enabled to randomize data distribution across folds.
   - For each fold:
     - Training and validation subsets are created.
     - A **K-Nearest Neighbors (KNN)** classifier is initialized with `n_neighbors=5`.
     - The model is trained on the training subset.
     - Predictions (`y_pred_k`) and probability scores (`y_prob_k`) are generated for the validation subset.

3. **Metric Calculation:**
   - Metrics are computed for each fold using the custom `calculate_metrics` function, which evaluates:
     - **Accuracy**
     - **Precision**
     - **Recall**
     - **F1-Score**
     - **Brier Score**
     - And other derived metrics
   - Metrics for each fold are stored in a list.

4. **Metrics Aggregation:**
   - The metrics for all folds are stored in a DataFrame for detailed analysis.
   - The average values of the metrics are computed to summarize the overall performance of the KNN classifier.

#### Output
- **Fold-wise Metrics:** Displays performance metrics for each fold, providing a fold-by-fold breakdown of the KNN model's performance.
- **Average Metrics:** Presents the mean values of all metrics across the 10 folds, offering an overall view of the KNN classifier's strengths and weaknesses.

#### Observations
The **KNN model** uses the proximity of data points to make predictions, which can work well with well-separated classes but may struggle with high-dimensional or imbalanced data. The evaluation includes standard classification metrics as well as probability calibration metrics like the **Brier Score**, providing a holistic assessment of the model's performance.

"""

# KNN Model
knn_metrics = []
for train_idx, val_idx in kf.split(X_train, y_train):
    X_train_k, X_val_k = X_train[train_idx], X_train[val_idx]
    y_train_k, y_val_k = y_train.iloc[train_idx], y_train.iloc[val_idx]

    knn_model = KNeighborsClassifier(n_neighbors=5)
    knn_model.fit(X_train_k, y_train_k)
    y_pred_k = knn_model.predict(X_val_k)
    y_prob_k = knn_model.predict_proba(X_val_k)[:, 1]

    metrics = calculate_metrics(y_val_k, y_pred_k, y_prob_k)
    knn_metrics.append(metrics)

knn_metrics_df = pd.DataFrame(knn_metrics)
print("\nKNN Metrics Per Fold:")
display(knn_metrics_df)

# Average Metrics for KNN
knn_avg_metrics = knn_metrics_df.mean()
print("\nKNN Average Metrics:")
print(knn_avg_metrics)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
from sklearn.metrics import roc_curve, auc

# Prepare data for LSTM (reshape X_train and X_test)
X_train_lstm = np.expand_dims(X_train, axis=-1)
X_test_lstm = np.expand_dims(X_test, axis=-1)

# Define LSTM model
def create_lstm_model(input_shape):
    model = Sequential()
    model.add(Bidirectional(LSTM(64, return_sequences=True, input_shape=input_shape)))
    model.add(Dropout(0.5))
    model.add(LSTM(32, return_sequences=False))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Cross-validation for LSTM
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
lstm_metrics = []

print("Starting LSTM Cross-Validation...\n")
for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):
    print(f"Training Fold {fold}...")

    # Split the data for this fold
    X_train_k, X_val_k = X_train_lstm[train_idx], X_train_lstm[val_idx]
    y_train_k, y_val_k = y_train.iloc[train_idx], y_train.iloc[val_idx]

    # Create and compile LSTM model
    lstm_model = create_lstm_model((X_train_k.shape[1], 1))

    # Early stopping
    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    # Train the model
    lstm_model.fit(X_train_k, y_train_k, validation_data=(X_val_k, y_val_k),
                   epochs=5, batch_size=64, callbacks=[early_stopping], verbose=0)

    # Predictions
    y_prob_k = lstm_model.predict(X_val_k).flatten()
    y_pred_k = (y_prob_k > 0.5).astype(int)

    # Calculate metrics
    metrics = calculate_metrics(y_val_k, y_pred_k, y_prob_k)
    lstm_metrics.append(metrics)

    # Print metrics for this fold
    print(f"Fold {fold} Metrics:")
    for metric_name, value in metrics.items():
        print(f"  - {metric_name}: {value:.4f}")
    print()

# Convert metrics to DataFrame for analysis
lstm_metrics_df = pd.DataFrame(lstm_metrics)
print("\nLSTM Metrics Per Fold:")
display(lstm_metrics_df)

# Average Metrics for LSTM
lstm_avg_metrics = lstm_metrics_df.mean()
print("\nLSTM Average Metrics:")
for metric_name, value in lstm_avg_metrics.items():
    print(f"  - {metric_name}: {value:.4f}")

# Compare All Models
print("\n----- Iteration-Wise Metrics Comparison -----\n")
algorithms = {
    "Random Forest": rf_metrics_df,
    "Naive Bayes": nb_metrics_df,
    "KNN": knn_metrics_df,
    "LSTM": lstm_metrics_df
}

for iteration in range(len(knn_metrics_df)):
    print(f"Iteration {iteration + 1}:")
    print("----- Metrics for all Algorithms in Iteration -----")
    comparison_table = {}
    for algo_name, metrics_df in algorithms.items():
        comparison_table[algo_name] = metrics_df.iloc[iteration]

    # Convert to DataFrame for display
    iteration_comparison_df = pd.DataFrame(comparison_table).T
    iteration_comparison_df.columns = rf_metrics_df.columns  # Use metric names as columns
    display(iteration_comparison_df)

# Compute average metrics for each algorithm
avg_metrics = {}
for algo_name, metrics_df in algorithms.items():
    avg_metrics[algo_name] = metrics_df.mean()

# Convert to DataFrame for display
avg_metrics_df = pd.DataFrame(avg_metrics).T
avg_metrics_df.columns = rf_metrics_df.columns  # Use metric names as columns
print("\n----- Average Metrics Across All Iterations -----\n")
display(avg_metrics_df)

# Combine average metrics for all algorithms
results_df = avg_metrics_df[['Accuracy', 'F1', 'Precision', 'ROC AUC']]
print("\nComparison of Average Metrics Across Models:")
display(results_df)

# Plot results
results_df.plot(kind='bar', figsize=(12, 8))
plt.title("Comparison of Average Metrics Across Models")
plt.ylabel("Scores")
plt.xticks(rotation=45)
plt.legend(loc="upper right")
plt.show()

"""### Comparison of Average Metrics Across All Algorithms

The table below summarizes the average performance metrics of the Random Forest, Naive Bayes, K-Nearest Neighbors (KNN), and LSTM models across all iterations:

| Algorithm        | TP     | TN     | FP     | FN     | TPR    | TNR    | Precision | F1     | Accuracy | Error Rate | Balanced Accuracy | TSS    | HSS    | Brier Score | Brier Skill Score | ROC AUC |
|------------------|---------|--------|--------|--------|--------|--------|-----------|--------|----------|------------|-------------------|--------|--------|-------------|-------------------|---------|
| **Random Forest** | 2350.8 | 1927.8 | 21.0   | 23.2   | 0.9902 | 0.9892 | 0.9911    | 0.9907 | 0.9898   | 0.0102     | 0.9897            | 0.9795 | 0.9795 | 0.0101      | 0.9593            | 0.9993  |
| **Naive Bayes**   | 1693.9 | 1049.2 | 899.6  | 680.1  | 0.7135 | 0.5384 | 0.6531    | 0.6820 | 0.6346   | 0.3654     | 0.6260            | 0.2519 | 0.2519 | 0.2310      | 0.0668            | 0.6879  |
| **KNN**           | 2352.3 | 1917.1 | 31.7   | 21.7   | 0.9909 | 0.9837 | 0.9867    | 0.9888 | 0.9876   | 0.0124     | 0.9873            | 0.9746 | 0.9746 | 0.0095      | 0.9617            | 0.9971  |
| **LSTM**          | 1920.1 | 1515.5 | 433.3  | 453.9  | 0.8088 | 0.7777 | 0.8166    | 0.8121 | 0.7948   | 0.2052     | 0.7932            | 0.5865 | 0.5865 | 0.1719      | 0.5244            | 0.8865  |

---

### Detailed Comparison

#### **Random Forest: The Best Performer**
- **Performance:** The Random Forest model outperformed all other algorithms across all metrics.
  - It achieved the highest **Accuracy (98.98%)**, **Precision (99.11%)**, **F1-Score (99.07%)**, and **Balanced Accuracy (98.97%)**.
  - Its **True Positive Rate (TPR)** and **True Negative Rate (TNR)** were nearly perfect at 99.02% and 98.92%, respectively.
- **Reliability:** The **Brier Score (0.0101)** indicates highly calibrated probability predictions, and the **Brier Skill Score (0.9593)** demonstrates its superior performance compared to a baseline model.
  - The **ROC AUC (0.9993)** confirms its ability to distinguish between classes effectively.
- **Conclusion:** Random Forest is the most consistent and reliable algorithm for this task.

#### **Naive Bayes: The Weakest Performer**
- **Performance:** Naive Bayes struggled significantly in comparison to the other models.
  - It achieved the lowest **Accuracy (63.46%)**, **Precision (65.31%)**, and **F1-Score (68.20%)**.
  - Its **True Positive Rate (TPR)** was moderate at 71.35%, but the **True Negative Rate (TNR)** was very low at 53.84%, indicating poor handling of negative samples.
- **Calibration:** The **Brier Score (0.2310)** and **Brier Skill Score (0.0668)** show that the probability estimates were poorly calibrated.
  - The **ROC AUC (0.6879)** indicates limited capability in distinguishing between classes.
- **Conclusion:** Naive Bayes, while simple and fast, is not suitable for this classification task due to its assumptions and inability to handle complex relationships in the data.

#### **K-Nearest Neighbors (KNN): A Close Second**
- **Performance:** KNN performed exceptionally well and closely matched the Random Forest model:
  - It achieved an **Accuracy (98.76%)**, **Precision (98.67%)**, and **F1-Score (98.88%)**, with slightly lower scores than Random Forest.
  - Its **True Positive Rate (TPR)** and **True Negative Rate (TNR)** were 99.09% and 98.37%, respectively, showcasing high reliability.
- **Calibration:** The **Brier Score (0.0095)** and **Brier Skill Score (0.9617)** indicate that KNN produced well-calibrated probability predictions.
  - The **ROC AUC (0.9971)** shows excellent class discrimination.
- **Conclusion:** KNN is a strong competitor to Random Forest, with near-identical performance but slightly less consistency.

#### **LSTM: Moderate Performance**
- **Performance:** The LSTM model showed moderate performance compared to the Random Forest and KNN models:
  - It achieved an **Accuracy (79.48%)**, **Precision (81.66%)**, and **F1-Score (81.21%)**.
  - Its **True Positive Rate (TPR)** was 80.88%, and the **True Negative Rate (TNR)** was 77.77%, reflecting reasonable performance but room for improvement.
- **Calibration:** The **Brier Score (0.1719)** and **Brier Skill Score (0.5244)** indicate that the LSTM's probability predictions were less reliable compared to Random Forest and KNN.
  - The **ROC AUC (0.8865)** shows moderate class discrimination.
- **Conclusion:** While LSTM leverages sequential data effectively, it falls short in terms of overall performance and consistency, potentially due to insufficient data or suboptimal hyperparameters.

---

### Final Verdict
- **Best Performer:** **Random Forest**, with the highest accuracy, precision, and reliability.
- **Runner-Up:** **KNN**, with comparable performance to Random Forest but slightly lower calibration and consistency.
- **Moderate Performer:** **LSTM**, showing potential but requiring further optimization.
- **Weakest Performer:** **Naive Bayes**, struggling with accuracy, calibration, and handling of negative samples.

Based on this analysis, **Random Forest** is the most suitable algorithm for the sentiment analysis task in this project.
"""

# Plotting ROC Curves for All Models
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Dictionary of models and their respective data
models = {
    'Random Forest': (rf_model, rf_metrics_df),
    'Naive Bayes': (nb_model, nb_metrics_df),
    'KNN': (knn_model, knn_metrics_df),
    'LSTM': (lstm_model, lstm_metrics_df)  # Added LSTM
}

plt.figure(figsize=(10, 8))
for name, (model, metrics_df) in models.items():
    if name == 'LSTM':
        # Get predicted probabilities for LSTM
        y_prob = lstm_model.predict(X_test_lstm).flatten()
    else:
        # Get predicted probabilities for other models
        y_prob = model.predict_proba(X_test)[:, 1]

    # Compute ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

# Plot diagonal line for random guessing
plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')
plt.title('ROC Curves for All Models')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid()
plt.show()